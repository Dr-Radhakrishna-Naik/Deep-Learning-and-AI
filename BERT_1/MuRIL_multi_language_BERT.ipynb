{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MuRIL_multi_language_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "What are all the languages it supports?\n",
        "\n",
        "MuRIL currently supports the following 17 languages:\n",
        "\n",
        "    Assamese\n",
        "    Bengali\n",
        "    English\n",
        "    Gujarati\n",
        "    Hindi\n",
        "    Kannada\n",
        "    Kashmiri\n",
        "    Malayalam\n",
        "    Marathi\n",
        "    Nepali\n",
        "    Oriya\n",
        "    Punjabi\n",
        "    Sanskrit\n",
        "    Sindhi\n",
        "    Tamil\n",
        "    Telugu\n",
        "    Urdu\n",
        "\n",
        "The training of MuRIL is very similar to Mulitlingual BERT with addition of translation and transliteration segment pairs in training. The MuRIL model is pre-trained on monolingual segments as well as parallel segments as detailed below :\n",
        "\n",
        "    Monolingual Data : Publicly available corpora from Wikipedia and Common Crawl for 17 Indian languages.\n",
        "    Translated Data : Translations of the above monolingual corpora using the Google NMT pipeline and publicly available PMINDIA corpus.\n",
        "    Transliterated Data : Transliterations of Wikipedia using the IndicTrans library and publicly available Dakshina dataset."
      ],
      "metadata": {
        "id": "VQZLPvqi_R5T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcXIIWvU7HXN",
        "outputId": "c25c3d49-e636-49bd-8007-3b29c18c5ec1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bert-for-tf2\n",
            "  Downloading bert-for-tf2-0.14.9.tar.gz (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 169 kB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading py-params-0.10.2.tar.gz (7.4 kB)\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading params-flow-0.8.2.tar.gz (22 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.64.0)\n",
            "Building wheels for collected packages: bert-for-tf2, params-flow, py-params\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.9-py3-none-any.whl size=30535 sha256=e526f6c6c79b4b16ce456a214de18201f6895d11f6e6b3ef553d3fd09ec5ae07\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/b6/e5/8c76ec779f54bc5c2f1b57d2200bb9c77616da83873e8acb53\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-py3-none-any.whl size=19472 sha256=3649a1e4ba4ed74c1211f526a8df45c0e8bb4dfb04ff19d26d0cb08786344be3\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/fc/d2/a44fff33af0f233d7def6e7de413006d57c10e10ad736fe8f5\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.10.2-py3-none-any.whl size=7911 sha256=b28d6eb3c51baa6df13f458d5976cf46ce440e2e2ab96d45ea76c360a57230e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/11/67/33cc51bbee127cb8fb2ba549cd29109b2f22da43ddf9969716\n",
            "Successfully built bert-for-tf2 params-flow py-params\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.9 params-flow-0.8.2 py-params-0.10.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.6 MB 6.8 MB/s \n",
            "\u001b[?25hCollecting tensorflow<2.10,>=2.9.0\n",
            "  Downloading tensorflow-2.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 511.7 MB 5.8 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text) (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.21.6)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.1.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.47.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (14.0.6)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.14.1)\n",
            "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
            "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[K     |████████████████████████████████| 438 kB 70.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (0.26.0)\n",
            "Collecting tensorboard<2.10,>=2.9\n",
            "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 51.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (57.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.6.3)\n",
            "Collecting gast<=0.4.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (21.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (3.17.3)\n",
            "Collecting keras<2.10.0,>=2.9.0rc0\n",
            "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 70.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (4.1.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (3.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.1.0)\n",
            "Collecting flatbuffers<2,>=1.12\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.8.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.0.9)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras, gast, flatbuffers, tensorflow, tensorflow-text\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0\n",
            "    Uninstalling flatbuffers-2.0:\n",
            "      Successfully uninstalled flatbuffers-2.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
            "Successfully installed flatbuffers-1.12 gast-0.4.0 keras-2.9.0 tensorboard-2.9.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0 tensorflow-text-2.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install bert-for-tf2\n",
        "!pip install tensorflow-text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from bert import bert_tokenization\n",
        "import numpy as np\n",
        "from scipy.spatial import distance"
      ],
      "metadata": {
        "id": "zgWwzk547ncY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The get_model function created here helps us download the MuRIL model from the Tensorflow Hub platform. This function accepts two inputs: the model_url and the maximum sequence length or the maximum number of tokens that can be imported at a time.\n",
        " TensorFlow Hub is a repository of trained machine learning models ready for fine-tuning and deployable anywhere."
      ],
      "metadata": {
        "id": "VHyx0Aca_uxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#To create token embedding,segment embedding and  positional embedding\n",
        "def get_model(model_url, max_seq_length):\n",
        "    inputs = dict(\n",
        "    input_word_ids=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32),\n",
        "    input_mask=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32),\n",
        "    input_type_ids=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32),\n",
        "    )\n",
        "  #Now let us build MuRIL model\n",
        "    muril_layer = hub.KerasLayer(model_url, trainable=True)\n",
        "    outputs = muril_layer(inputs)\n",
        "\n",
        "    assert 'sequence_output' in outputs\n",
        "    assert 'pooled_output' in outputs\n",
        "    assert 'encoder_outputs' in outputs\n",
        "    assert 'default' in outputs\n",
        "    return tf.keras.Model(inputs=inputs,outputs=outputs[\"pooled_output\"]), muril_layer"
      ],
      "metadata": {
        "id": "YxGTjHFL73gM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 128\n",
        "muril_model, muril_layer = get_model(\n",
        "    model_url=\"https://tfhub.dev/google/MuRIL/1\", max_seq_length=max_seq_length)"
      ],
      "metadata": {
        "id": "pmDhxKyIuWbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A vocabulary file in the form of Numpy array and a full tokenizer for BERT are created here."
      ],
      "metadata": {
        "id": "cZy-zuqOAj6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_file = muril_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = muril_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = bert_tokenization.FullTokenizer(vocab_file, do_lower_case)"
      ],
      "metadata": {
        "id": "GNb3_ssg90zY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to create the metadata for the input sequence.\n",
        "Here Token embedding,segment embedding and postion embedding are created"
      ],
      "metadata": {
        "id": "ZkOFkbZ_BK3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_input(input_strings, tokenizer, max_seq_length):\n",
        "  input_ids_all, input_mask_all, input_type_ids_all = [], [], []\n",
        "  for input_string in input_strings:\n",
        "    input_tokens = [\"[CLS]\"] + tokenizer.tokenize(input_string) + [\"[SEP]\"]\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
        "    sequence_length = min(len(input_ids), max_seq_length)\n",
        "    \n",
        "    if len(input_ids) >= max_seq_length:\n",
        "      input_ids = input_ids[:max_seq_length]\n",
        "    else:\n",
        "      input_ids = input_ids + [0] * (max_seq_length - len(input_ids))\n",
        "\n",
        "    input_mask = [1] * sequence_length + [0] * (max_seq_length - sequence_length)\n",
        "\n",
        "    input_ids_all.append(input_ids)\n",
        "    input_mask_all.append(input_mask)\n",
        "    input_type_ids_all.append([0] * max_seq_length)\n",
        "  \n",
        "  return np.array(input_ids_all), np.array(input_mask_all), np.array(input_type_ids_all)"
      ],
      "metadata": {
        "id": "y-M7CulQ9-NJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The encode function created here is obviously the final part of this experiment. It accepts the input string, creates the metadata, and is then passed into the MuRIL model."
      ],
      "metadata": {
        "id": "6tcbQOFfBtQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(input_text):\n",
        "  input_ids, input_mask, input_type_ids = create_input(input_text, \n",
        "                                                       tokenizer, \n",
        "                                                       max_seq_length)\n",
        "  inputs = dict(\n",
        "      input_word_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      input_type_ids=input_type_ids,\n",
        "  )\n",
        "  return muril_model(inputs)"
      ],
      "metadata": {
        "id": "B4w8yJyE-PvP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This list is now passed into the encode function to get the final BERT representations of the words."
      ],
      "metadata": {
        "id": "jEjnPzeeB61A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"दोस्त\", \"मित्र\", \"शत्रु\"]"
      ],
      "metadata": {
        "id": "3fUBeuLG-TBw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = encode(sentences)"
      ],
      "metadata": {
        "id": "kUKcxXRb-YvK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Knowing the meaning of the three words specified in the list, let us check the distance between each of the words i.e. the first two words, “dost” and “mitr”, more or less means the same, meaning that the distance between these words in a high dimensional plane is as less as possible."
      ],
      "metadata": {
        "id": "R-H-WjmHCEnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dst_1 = distance.euclidean(np.array(embeddings[0]), \n",
        "                           np.array(embeddings[1]))\n",
        "print(\"Distance between {} & {} is {}\".format(sentences[0],\n",
        "                                                sentences[1],\n",
        "                                                dst_1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdaOkri5-moq",
        "outputId": "801b3f15-c614-4dc2-ba14-e7da2dcf4014"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distance between दोस्त & मित्र is 0.009007968939840794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whereas the distance between the second and third words, “mitr” and “shatru”, seems comparatively higher."
      ],
      "metadata": {
        "id": "-RB5ClRJCPcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dst_2 = distance.euclidean(np.array(embeddings[1]), \n",
        "                           np.array(embeddings[2]))\n",
        "print(\"Distance between {} & {} is {}\".format(sentences[1],\n",
        "                                                sentences[2],\n",
        "                                                dst_2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1GU35zD-ptE",
        "outputId": "d58149dc-a1dd-4cfe-c7a0-ee5f16ef0b65"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distance between मित्र & शत्रु is 0.011569418013095856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dst_2 > dst_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZcS2A5z-vJ0",
        "outputId": "285532a4-f3ee-48d2-830e-c4bcae2ca558"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "code_mix_sentences = [\"मै घर जाऊंगा\",\"मै घर जा रही हूँ\",\"i am going home\",'main ghar ja raha hoon','apka naam kya hai']"
      ],
      "metadata": {
        "id": "douwMyvgEWxV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "code_mix_embedding = encode(code_mix_sentences)"
      ],
      "metadata": {
        "id": "d48IaWK8EahV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "hyH8i9vLE1Wy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dst_1 = cosine_similarity(np.array(code_mix_embedding[0]).reshape(1,-1), \n",
        "                           np.array(code_mix_embedding[1]).reshape(1,-1))\n",
        "print(\"SIMILARITY between {} & {} is {}\".format(code_mix_sentences[0],\n",
        "                                                code_mix_sentences[1],\n",
        "                                                dst_1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hpk44USIEeGM",
        "outputId": "9ff528c5-b0e2-406c-e77f-5511af6caa65"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SIMILARITY between मै घर जाऊंगा & मै घर जा रही हूँ is [[0.9997084]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dst_1 = cosine_similarity(np.array(code_mix_embedding[0]).reshape(1,-1), \n",
        "                           np.array(code_mix_embedding[2]).reshape(1,-1))\n",
        "print(\"SIMILARITY between {} & {} is {}\".format(code_mix_sentences[0],\n",
        "                                                code_mix_sentences[2],\n",
        "                                                dst_1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2XYa0M9E5Lb",
        "outputId": "7e0f974a-1e6f-470a-9351-2bb338e8a30b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SIMILARITY between मै घर जाऊंगा & i am going home is [[0.99974626]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dst_1 = cosine_similarity(np.array(code_mix_embedding[0]).reshape(1,-1), \n",
        "                           np.array(code_mix_embedding[4]).reshape(1,-1))\n",
        "print(\"SIMILARITY between {} & {} is {}\".format(code_mix_sentences[0],\n",
        "                                                code_mix_sentences[4],\n",
        "                                                dst_1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2254SORFS7S",
        "outputId": "2edb3b5c-929c-4e85-e934-05180cc26618"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SIMILARITY between मै घर जाऊंगा & apka naam kya hai is [[0.99927735]]\n"
          ]
        }
      ]
    }
  ]
}